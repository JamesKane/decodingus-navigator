package com.decodingus.analysis

import java.io.{File, PrintWriter}
import java.nio.file.Path
import scala.util.Using

/**
 * Processor for collecting unified read metrics from BAM/CRAM files.
 *
 * Replaces MultipleMetricsProcessor (GATK CollectMultipleMetrics) which has
 * a hard dependency on R for histogram generation.
 *
 * Uses HTSJDK directly for a single-pass collection of:
 * - Read counts (total, PF, aligned, paired, proper pairs)
 * - Read length statistics
 * - Insert size distribution and statistics
 * - Pair orientation
 * - Chimera rate
 */
class UnifiedMetricsProcessor {

  private val ARTIFACT_SUBDIR_NAME = "read_metrics"
  private val walker = new UnifiedMetricsWalker()

  /**
   * Process a BAM/CRAM file to collect read-level metrics.
   *
   * @param bamPath Path to BAM/CRAM file
   * @param referencePath Path to reference genome
   * @param onProgress Progress callback
   * @param artifactContext Optional context for organizing output artifacts
   * @return Either error or ReadMetrics
   */
  def process(
    bamPath: String,
    referencePath: String,
    onProgress: (String, Double, Double) => Unit,
    artifactContext: Option[ArtifactContext] = None
  ): Either[Throwable, ReadMetrics] = {

    onProgress("Collecting read metrics...", 0.0, 1.0)

    // Verify input file exists
    if (!new File(bamPath).exists()) {
      return Left(new RuntimeException(s"BAM/CRAM file not found: $bamPath"))
    }

    // Progress adapter
    val progressAdapter: (String, Long, Long) => Unit = (msg, current, total) => {
      val fraction = if (total > 0) current.toDouble / total else 0.0
      onProgress(msg, fraction * 0.9, 1.0) // Reserve last 10% for finalization
    }

    walker.collectReadMetrics(bamPath, referencePath, progressAdapter) match {
      case Right(metrics) =>
        // Write metrics to artifact directory if available
        artifactContext.foreach { ctx =>
          val outputDir = ctx.getSubdir(ARTIFACT_SUBDIR_NAME)
          writeMetricsFile(outputDir, metrics)
          writeHistogram(outputDir, "read_length_histogram.tsv", "read_length", metrics.readLengthHistogram)
          writeHistogram(outputDir, "insert_size_histogram.tsv", "insert_size", metrics.insertSizeHistogram)
        }

        onProgress("Read metrics collection complete.", 1.0, 1.0)
        Right(metrics)

      case Left(error) =>
        Left(new RuntimeException(error))
    }
  }

  /**
   * Write metrics summary to a text file (similar to Picard format).
   */
  private def writeMetricsFile(outputDir: Path, metrics: ReadMetrics): Unit = {
    val metricsFile = outputDir.resolve("read_metrics.txt").toFile
    outputDir.toFile.mkdirs()

    Using.resource(new PrintWriter(metricsFile)) { writer =>
      writer.println("## UNIFIED READ METRICS")
      writer.println("## Generated by UnifiedMetricsProcessor (no R dependency)")
      writer.println()

      writer.println("## ALIGNMENT SUMMARY METRICS")
      writer.println(s"TOTAL_READS\t${metrics.totalReads}")
      writer.println(s"PF_READS\t${metrics.pfReads}")
      writer.println(s"PF_READS_ALIGNED\t${metrics.pfReadsAligned}")
      writer.println(f"PCT_PF_READS_ALIGNED\t${metrics.pctPfReadsAligned}%.6f")
      writer.println(s"READS_ALIGNED_IN_PAIRS\t${metrics.readsAlignedInPairs}")
      writer.println(f"PCT_READS_ALIGNED_IN_PAIRS\t${metrics.pctReadsAlignedInPairs}%.6f")
      writer.println(s"PF_PROPER_PAIRS\t${metrics.properPairs}")
      writer.println(f"PCT_PROPER_PAIRS\t${metrics.pctProperPairs}%.6f")
      writer.println(f"MEAN_READ_LENGTH\t${metrics.meanReadLength}%.2f")
      writer.println(s"MAX_READ_LENGTH\t${metrics.maxReadLength}")
      writer.println(f"PCT_CHIMERAS\t${metrics.pctChimeras}%.6f")
      writer.println(f"MEAN_MAPPING_QUALITY\t${metrics.meanMappingQuality}%.2f")
      writer.println()

      writer.println("## READ LENGTH METRICS")
      writer.println(f"MEDIAN_READ_LENGTH\t${metrics.medianReadLength}%.1f")
      writer.println(f"MEAN_READ_LENGTH\t${metrics.meanReadLength}%.2f")
      writer.println(f"STANDARD_DEVIATION\t${metrics.stdReadLength}%.2f")
      writer.println(s"MIN_READ_LENGTH\t${metrics.minReadLength}")
      writer.println(s"MAX_READ_LENGTH\t${metrics.maxReadLength}")
      writer.println()

      writer.println("## INSERT SIZE METRICS")
      writer.println(f"MEDIAN_INSERT_SIZE\t${metrics.medianInsertSize}%.1f")
      writer.println(f"MEAN_INSERT_SIZE\t${metrics.meanInsertSize}%.2f")
      writer.println(f"STANDARD_DEVIATION\t${metrics.stdInsertSize}%.2f")
      writer.println(s"MIN_INSERT_SIZE\t${metrics.minInsertSize}")
      writer.println(s"MAX_INSERT_SIZE\t${metrics.maxInsertSize}")
      writer.println(s"PAIR_ORIENTATION\t${metrics.pairOrientation}")
    }
  }

  /**
   * Write a histogram to a TSV file.
   * Can be used for visualization without R.
   *
   * @param outputDir Directory to write to
   * @param filename Output filename
   * @param columnName Name for the value column (e.g., "insert_size", "read_length")
   * @param histogram The histogram data
   */
  private def writeHistogram(outputDir: Path, filename: String, columnName: String, histogram: Map[Int, Long]): Unit = {
    if (histogram.isEmpty) return

    val histFile = outputDir.resolve(filename).toFile

    Using.resource(new PrintWriter(histFile)) { writer =>
      writer.println(s"$columnName\tcount")
      histogram.toSeq.sortBy(_._1).foreach { case (value, count) =>
        writer.println(s"$value\t$count")
      }
    }
  }
}
